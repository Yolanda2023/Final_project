{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e55eea6",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb4480",
   "metadata": {},
   "source": [
    "## 1. Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c486709c",
   "metadata": {},
   "source": [
    "Programmatically download the Yellow Taxi trip data\n",
    "\n",
    "Using the re module, write a regular expression to help pull out the desired links for Yellow Taxi Parquet files\n",
    "\n",
    "Use the 3rd-party packages requests and  BeautifulSoup to programmatically download the Yellow Taxi Parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eff036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "import re\n",
    "import unittest\n",
    "from typing import List\n",
    "from unittest.mock import patch\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0608e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for taxi data\n",
    "taxi_data_url = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098c000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def download_file(url: str, local_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Download a file from the given URL and save it to the specified local file.\n",
    "\n",
    "    :param url: The URL of the file to download.\n",
    "    :param local_filename: The path to save the downloaded file.\n",
    "    :return: The path of the saved file.\n",
    "    \"\"\"\n",
    "    # Send an HTTP request to the URL\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        # Raise an exception if the request fails\n",
    "        r.raise_for_status()\n",
    "        # Save the file to the specified path\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            # Write the content of the response in chunks\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "def get_yellow_taxi_parquet_links(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get yellow taxi parquet links from the BeautifulSoup object.\n",
    "\n",
    "    :param soup: BeautifulSoup object containing the parsed HTML content.\n",
    "    :return: A list of yellow taxi parquet links.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the links\n",
    "    yellow_taxi_parquet_links = []\n",
    "    # Iterate through all 'a' tags with 'href' attribute\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        # Check if the link matches the regex pattern\n",
    "        if re.match(r'.*yellow_tripdata_(2009|2010|2011|2012|2013|2014|2015).*\\.parquet', href):\n",
    "            # Check if the month is between January and June for 2015 data\n",
    "            if \"2015\" in href:\n",
    "                date_part = re.search(r\"(\\d{4})-(\\d{2})\", href)\n",
    "                if date_part:\n",
    "                    year, month = int(date_part.group(1)), int(date_part.group(2))\n",
    "                    if year == 2015 and month > 6:\n",
    "                        continue\n",
    "            # Add the link to the list\n",
    "            yellow_taxi_parquet_links.append(href)\n",
    "    return yellow_taxi_parquet_links\n",
    "\n",
    "def test_download_file():\n",
    "    import tempfile\n",
    "    test_url = \"https://www.example.com/\"\n",
    "    with tempfile.NamedTemporaryFile() as temp_file:\n",
    "        local_path = temp_file.name\n",
    "        assert download_file(test_url, local_path) == local_path\n",
    "        assert os.path.getsize(local_path) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3852e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an HTTP request to the taxi data URL\n",
    "response = requests.get(taxi_data_url)\n",
    "# Parse the HTML content of the response\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "# Get the list of yellow taxi parquet links\n",
    "yellow_taxi_parquet_links = get_yellow_taxi_parquet_links(soup)\n",
    "\n",
    "# Define the directory to store the downloaded files\n",
    "yellow_taxi_data_dir = \"yellow_taxi_data\"\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(yellow_taxi_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0af0dfd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading yellow_tripdata_2015-01.parquet ...\n",
      "Downloading yellow_tripdata_2015-02.parquet ...\n",
      "Downloading yellow_tripdata_2015-03.parquet ...\n",
      "Downloading yellow_tripdata_2015-04.parquet ...\n",
      "Downloading yellow_tripdata_2015-05.parquet ...\n",
      "Downloading yellow_tripdata_2015-06.parquet ...\n",
      "Downloading yellow_tripdata_2014-01.parquet ...\n",
      "Downloading yellow_tripdata_2014-02.parquet ...\n",
      "Downloading yellow_tripdata_2014-03.parquet ...\n",
      "Downloading yellow_tripdata_2014-04.parquet ...\n",
      "Downloading yellow_tripdata_2014-05.parquet ...\n",
      "Downloading yellow_tripdata_2014-06.parquet ...\n",
      "Downloading yellow_tripdata_2014-07.parquet ...\n",
      "Downloading yellow_tripdata_2014-08.parquet ...\n",
      "Downloading yellow_tripdata_2014-09.parquet ...\n",
      "Downloading yellow_tripdata_2014-10.parquet ...\n",
      "Downloading yellow_tripdata_2014-11.parquet ...\n",
      "Downloading yellow_tripdata_2014-12.parquet ...\n",
      "Downloading yellow_tripdata_2013-01.parquet ...\n",
      "Downloading yellow_tripdata_2013-02.parquet ...\n",
      "Downloading yellow_tripdata_2013-03.parquet ...\n",
      "Downloading yellow_tripdata_2013-04.parquet ...\n",
      "Downloading yellow_tripdata_2013-05.parquet ...\n",
      "Downloading yellow_tripdata_2013-06.parquet ...\n",
      "Downloading yellow_tripdata_2013-07.parquet ...\n",
      "Downloading yellow_tripdata_2013-08.parquet ...\n",
      "Downloading yellow_tripdata_2013-09.parquet ...\n",
      "Downloading yellow_tripdata_2013-10.parquet ...\n",
      "Downloading yellow_tripdata_2013-11.parquet ...\n",
      "Downloading yellow_tripdata_2013-12.parquet ...\n",
      "Downloading yellow_tripdata_2012-01.parquet ...\n",
      "Downloading yellow_tripdata_2012-02.parquet ...\n",
      "Downloading yellow_tripdata_2012-03.parquet ...\n",
      "Downloading yellow_tripdata_2012-04.parquet ...\n",
      "Downloading yellow_tripdata_2012-05.parquet ...\n",
      "Downloading yellow_tripdata_2012-06.parquet ...\n",
      "Downloading yellow_tripdata_2012-07.parquet ...\n",
      "Downloading yellow_tripdata_2012-08.parquet ...\n",
      "Downloading yellow_tripdata_2012-09.parquet ...\n",
      "Downloading yellow_tripdata_2012-10.parquet ...\n",
      "Downloading yellow_tripdata_2012-11.parquet ...\n",
      "Downloading yellow_tripdata_2012-12.parquet ...\n",
      "Downloading yellow_tripdata_2011-01.parquet ...\n",
      "Downloading yellow_tripdata_2011-02.parquet ...\n",
      "Downloading yellow_tripdata_2011-03.parquet ...\n",
      "Downloading yellow_tripdata_2011-04.parquet ...\n",
      "Downloading yellow_tripdata_2011-05.parquet ...\n",
      "Downloading yellow_tripdata_2011-06.parquet ...\n",
      "Downloading yellow_tripdata_2011-07.parquet ...\n",
      "Downloading yellow_tripdata_2011-08.parquet ...\n",
      "Downloading yellow_tripdata_2011-09.parquet ...\n",
      "Downloading yellow_tripdata_2011-10.parquet ...\n",
      "Downloading yellow_tripdata_2011-11.parquet ...\n",
      "Downloading yellow_tripdata_2011-12.parquet ...\n",
      "Downloading yellow_tripdata_2010-01.parquet ...\n",
      "Downloading yellow_tripdata_2010-02.parquet ...\n",
      "Downloading yellow_tripdata_2010-03.parquet ...\n",
      "Downloading yellow_tripdata_2010-04.parquet ...\n",
      "Downloading yellow_tripdata_2010-05.parquet ...\n",
      "Downloading yellow_tripdata_2010-06.parquet ...\n",
      "Downloading yellow_tripdata_2010-07.parquet ...\n",
      "Downloading yellow_tripdata_2010-08.parquet ...\n",
      "Downloading yellow_tripdata_2010-09.parquet ...\n",
      "Downloading yellow_tripdata_2010-10.parquet ...\n",
      "Downloading yellow_tripdata_2010-11.parquet ...\n",
      "Downloading yellow_tripdata_2010-12.parquet ...\n",
      "Downloading yellow_tripdata_2009-01.parquet ...\n",
      "Downloading yellow_tripdata_2009-02.parquet ...\n",
      "Downloading yellow_tripdata_2009-03.parquet ...\n",
      "Downloading yellow_tripdata_2009-04.parquet ...\n",
      "Downloading yellow_tripdata_2009-05.parquet ...\n",
      "Downloading yellow_tripdata_2009-06.parquet ...\n",
      "Downloading yellow_tripdata_2009-07.parquet ...\n",
      "Downloading yellow_tripdata_2009-08.parquet ...\n",
      "Downloading yellow_tripdata_2009-09.parquet ...\n",
      "Downloading yellow_tripdata_2009-10.parquet ...\n",
      "Downloading yellow_tripdata_2009-11.parquet ...\n",
      "Downloading yellow_tripdata_2009-12.parquet ...\n"
     ]
    }
   ],
   "source": [
    "# Download and save each file from the list of links\n",
    "for url in yellow_taxi_parquet_links:\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "    local_path = os.path.join(yellow_taxi_data_dir, file_name)\n",
    "    print(f\"Downloading {file_name} ...\")\n",
    "    download_file(url, local_path)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62554e",
   "metadata": {},
   "source": [
    "## 2. Cleaning & filtering:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb656b35",
   "metadata": {},
   "source": [
    "looking up the latitude and longitude for some months where only location IDs are given for pickups and dropoffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d881a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "def read_zones_from_shapefile(shapefile_path: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Read the shapefile and return the GeoDataFrame with the required columns.\n",
    "\n",
    "    :param shapefile_path: The path to the shapefile.\n",
    "    :return: A GeoDataFrame with the required columns.\n",
    "    \"\"\"\n",
    "    # Read the shapefile\n",
    "    zones = gpd.read_file(shapefile_path)\n",
    "    \n",
    "    # Convert the coordinate reference system\n",
    "    zones = zones.to_crs(4326)\n",
    "    \n",
    "    # Calculate the longitude and latitude for each zone\n",
    "    zones['Lon'] = zones.centroid.x\n",
    "    zones['Lat'] = zones.centroid.y\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    zones = zones.drop(columns=['OBJECTID', \"Shape_Leng\", 'Shape_Area', \"zone\", 'borough', 'geometry'])\n",
    "    \n",
    "    return zones\n",
    "\n",
    "\n",
    "def test_read_zones_from_shapefile():\n",
    "    import os\n",
    "    test_shapefile_path = \"path/to/your/test/shapefile.shp\"\n",
    "    \n",
    "    if os.path.exists(test_shapefile_path):\n",
    "        zones = read_zones_from_shapefile(test_shapefile_path)\n",
    "        assert isinstance(zones, gpd.GeoDataFrame)\n",
    "        assert \"Lon\" in zones.columns\n",
    "        assert \"Lat\" in zones.columns\n",
    "    else:\n",
    "        print(\"Test shapefile not found, skipping test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a98ef33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LocationID        Lon        Lat\n",
      "0             1 -74.174000  40.691831\n",
      "1             2 -73.831299  40.616745\n",
      "2             3 -73.847422  40.864474\n",
      "3             4 -73.976968  40.723752\n",
      "4             5 -74.188484  40.552659\n",
      "..          ...        ...        ...\n",
      "258         259 -73.852215  40.897932\n",
      "259         260 -73.906306  40.744235\n",
      "260         261 -74.013023  40.709139\n",
      "261         262 -73.946510  40.775932\n",
      "262         263 -73.951010  40.778766\n",
      "\n",
      "[263 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2716125885.py:19: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones['Lon'] = zones.centroid.x\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2716125885.py:20: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones['Lat'] = zones.centroid.y\n"
     ]
    }
   ],
   "source": [
    "# Set the path to shapefile\n",
    "shapefile_path = \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/taxi_zones/taxi_zones.shp\"\n",
    "\n",
    "# Read the GeoDataFrame from the shapefile\n",
    "zones = read_zones_from_shapefile(shapefile_path)\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "print(zones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2e8b8",
   "metadata": {},
   "source": [
    "###  Calculate distance（Missing data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4d47f",
   "metadata": {},
   "source": [
    "Add to the datasets missing information that can be calculated. Mainly: the distance between a trip’s starting point and ending point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3aef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "def calculate_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the distance between two points on Earth, given their latitudes and longitudes.\n",
    "\n",
    "    :param lat1: Latitude of the first point.\n",
    "    :param lon1: Longitude of the first point.\n",
    "    :param lat2: Latitude of the second point.\n",
    "    :param lon2: Longitude of the second point.\n",
    "    :return: The distance between the two points in kilometers.\n",
    "    \"\"\"\n",
    "    # Approximate radius of Earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Calculate the differences in latitudes and longitudes\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    # Apply the Haversine formula\n",
    "    a = (math.sin(dlat / 2)) ** 2 + math.cos(lat1) * math.cos(lat2) * (math.sin(dlon / 2)) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # Calculate the distance in kilometers\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def test_calculate_distance():\n",
    "    lat1, lon1 = 40.7128, -74.0060  # New York City\n",
    "    lat2, lon2 = 51.5074, -0.1278  # London\n",
    "\n",
    "    distance = calculate_distance(lat1, lon1, lat2, lon2)\n",
    "    assert isinstance(distance, float)\n",
    "    assert round(distance) == 5571  # The distance between New York City and London is approximately 5571 km\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc381e8",
   "metadata": {},
   "source": [
    "### Processing uber rides sample(csv)+Missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c19351",
   "metadata": {},
   "source": [
    "removing unnecessary columns and only keeping columns needed to answer questions in the other parts of this project\n",
    "\n",
    "removing invalid data points\n",
    "\n",
    "normalizing column names\n",
    "\n",
    "normalizing and using appropriate column types for the respective data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc22d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_uber_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the Uber data from the given CSV file and return a DataFrame with the required columns.\n",
    "\n",
    "    :param csv_path: The path to the CSV file.\n",
    "    :return: A DataFrame with the required columns.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    uber_data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Calculate distance and add it to the DataFrame\n",
    "    uber_data['Distance'] = uber_data.apply(lambda row: calculate_distance(row['pickup_latitude'], row['pickup_longitude'],\n",
    "                                                                  row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
    "\n",
    "    # Define coordinate box boundaries\n",
    "    lat_min, lat_max = 40.560445, 40.908524\n",
    "    lng_min, lng_max = -74.242330, -73.717047\n",
    "\n",
    "    # Filter trips within the coordinate box\n",
    "    uber_data = uber_data[(uber_data['pickup_latitude'] >= lat_min) & (uber_data['pickup_latitude'] <= lat_max) &\n",
    "                (uber_data['dropoff_latitude'] >= lat_min) & (uber_data['dropoff_latitude'] <= lat_max) &\n",
    "                (uber_data['pickup_longitude'] >= lng_min) & (uber_data['pickup_longitude'] <= lng_max) &\n",
    "                (uber_data['dropoff_longitude'] >= lng_min) & (uber_data['dropoff_longitude'] <= lng_max)]\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    uber_data.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    # Drop trips with zero passenger count or fare amount\n",
    "    uber_data = uber_data[(uber_data[\"passenger_count\"] != 0) & (uber_data[\"fare_amount\"] != 0)]\n",
    "\n",
    "    # Drop trips with zero distance\n",
    "    uber_data = uber_data[uber_data[\"Distance\"] != 0]\n",
    "\n",
    "    # Convert 'pickup_datetime' column to pandas datetime object and extract hour and day of week\n",
    "    uber_data['pickup_datetime'] = pd.to_datetime(uber_data['pickup_datetime']).dt.tz_localize(None)\n",
    "    uber_data['pickup_hour'] = uber_data['pickup_datetime'].dt.hour\n",
    "    uber_data['day_of_week'] = uber_data['pickup_datetime'].dt.dayofweek\n",
    "\n",
    "    # Select required columns and rename them\n",
    "    uber_data = uber_data[['pickup_datetime', 'pickup_hour', 'day_of_week', 'pickup_longitude', 'pickup_latitude',\n",
    "                           'dropoff_longitude', 'dropoff_latitude', \"Distance\"]]\n",
    "    uber_data.rename(columns={\n",
    "            'pickup_longitude': 'Start_Lon',\n",
    "            'pickup_latitude': 'Start_Lat',\n",
    "            'dropoff_longitude': 'End_Lon',\n",
    "            'dropoff_latitude': 'End_Lat',\n",
    "            'Distance': 'Trip_Distance',\n",
    "            'pickup_datetime': 'Pickup_Datetime'\n",
    "        }, inplace=True)\n",
    "\n",
    "    # Sort by pickup datetime and reset index\n",
    "    uber_data = uber_data.sort_values(by='Pickup_Datetime').reset_index(drop=True)\n",
    "\n",
    "    return uber_data\n",
    "\n",
    "def test_process_uber_data():\n",
    "    test_csv_path = \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/uber_rides_sample.csv\"\n",
    "    \n",
    "    if os.path.exists(test_csv_path):\n",
    "        processed_data = process_uber_data(test_csv_path)\n",
    "        assert isinstance(processed_data, pd.DataFrame)\n",
    "        assert \"Pickup_Datetime\" in processed_data.columns\n",
    "        assert \"Start_Lon\" in processed_data.columns\n",
    "        assert \"Start_Lat\" in processed_data.columns\n",
    "    else:\n",
    "        print(\"Test CSV file not found, skipping test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cfb11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the processed data to a new CSV file\n",
    "def save_processed_data(processed_data: pd.DataFrame, output_csv_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the processed Uber data to a new CSV file.\n",
    "\n",
    "    :param processed_data: The DataFrame containing the processed data.\n",
    "    :param output_csv_path: The path to the output CSV file.\n",
    "    \"\"\"\n",
    "    processed_data.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "def test_save_processed_data():\n",
    "    test_output_csv_path = \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/output_uber_rides_sample.csv\"\n",
    "    test_processed_data = pd.DataFrame({\n",
    "        'Pickup_Datetime': [pd.Timestamp('2015-05-17 09:43:00')],\n",
    "        'pickup_hour': [9],\n",
    "        'day_of_week': [6],\n",
    "        'Start_Lon': [-73.9871],\n",
    "        'Start_Lat': [40.7339],\n",
    "        'End_Lon': [-73.9894],\n",
    "        'End_Lat': [40.7411],\n",
    "        'Trip_Distance': [0.817]},\n",
    "        index=[0])\n",
    "\n",
    "    save_processed_data(test_processed_data, test_output_csv_path)\n",
    "    assert os.path.exists(test_output_csv_path)\n",
    "\n",
    "    loaded_data = pd.read_csv(test_output_csv_path)\n",
    "    print(\"Loaded Data:\\n\", loaded_data)\n",
    "    print(\"Expected Processed Data:\\n\", test_processed_data)\n",
    "    assert loaded_data.equals(test_processed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c6ebd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of the functions:\n",
    "input_csv_path = \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/uber_rides_sample.csv\"\n",
    "output_csv_path = \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/processed_uber_rides_sample.csv\"\n",
    "\n",
    "uber_data = process_uber_data(input_csv_path)\n",
    "save_processed_data(uber_data, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc4eb7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test functions here to check if everything is working correctly.\n",
    "#test_calculate_distance()\n",
    "test_process_uber_data()\n",
    "#test_save_processed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aee1df",
   "metadata": {},
   "source": [
    "### Processing yellow_taxi files+Sampling+Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9d676",
   "metadata": {},
   "source": [
    "Cleaning & filtering:\n",
    "\n",
    "removing unnecessary columns and only keeping columns needed to answer questions in the other parts of this project\n",
    "\n",
    "removing invalid data points\n",
    "\n",
    "normalizing column names\n",
    "\n",
    "normalizing and using appropriate column types for the respective data\n",
    "\n",
    "for Uber and Yellow Taxi data, removing trips that start and/or end outside of the following latitude/longitude coordinate box: (40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "\n",
    "Sampling: Each month of Yellow Taxi data contains millions of trips. However, the provided Uber dataset is only a sampling of all data. Therefore, you will need to generate a sampling of Yellow Taxi data that’s roughly equal to the sample size of the Uber dataset. \n",
    "\n",
    "Missing data: To help answer the questions later in the project, you will need to add to the datasets missing information that can be calculated. Mainly: the distance between a trip’s starting point and ending point \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35803c05",
   "metadata": {},
   "source": [
    "#### clean the data for year 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fab0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_2009(raw_dataframe_2009: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the input DataFrame for year 2009 and clean the data.\n",
    "\n",
    "    :param raw_dataframe_2009: The DataFrame containing the raw data for year 2009.\n",
    "    :return: A cleaned DataFrame with the required columns.\n",
    "    \"\"\"\n",
    "    # Drop trips with zero passenger count\n",
    "    cleaned_df_2009 = raw_dataframe_2009[raw_dataframe_2009['Passenger_Count'] != 0]\n",
    "    # Drop trips with no fare\n",
    "    cleaned_df_2009 = cleaned_df_2009[cleaned_df_2009['Fare_Amt'] != 0]\n",
    "    cleaned_df_2009 = cleaned_df_2009[cleaned_df_2009['Tip_Amt'] != 0]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    cleaned_df_2009 = cleaned_df_2009.drop(columns=[\"vendor_name\", \"Passenger_Count\", \"Rate_Code\",\"store_and_forward\", \"Payment_Type\", \n",
    "                          \"surcharge\", \"mta_tax\", \"Tolls_Amt\", \"Total_Amt\",\"Trip_Dropoff_DateTime\",\"Fare_Amt\"])\n",
    "\n",
    "    # Filter out trips that start and/or end outside of the defined latitude/longitude bounds\n",
    "    lat_min, lat_max = 40.560445, 40.908524\n",
    "    lng_min, lng_max = -74.242330, -73.717047\n",
    "    cleaned_df_2009 = cleaned_df_2009[(cleaned_df_2009['Start_Lat'] >= lat_min) & (cleaned_df_2009['Start_Lat'] <= lat_max) &\n",
    "            (cleaned_df_2009['End_Lat'] >= lat_min) & (cleaned_df_2009['End_Lat'] <= lat_max) &\n",
    "            (cleaned_df_2009['Start_Lon'] >= lng_min) & (cleaned_df_2009['Start_Lon'] <= lng_max) &\n",
    "            (cleaned_df_2009['End_Lon'] >= lng_min) & (cleaned_df_2009['End_Lon'] <= lng_max)]\n",
    "\n",
    "    # Filter out rows with the same pickup and dropoff location\n",
    "    cleaned_df_2009 = cleaned_df_2009[(cleaned_df_2009['Start_Lon'] != cleaned_df_2009['End_Lon']) &(cleaned_df_2009['Start_Lat'] != cleaned_df_2009['End_Lat'])]\n",
    "\n",
    "    cleaned_df_2009.rename(columns = {'Trip_Pickup_DateTime':'Pickup_Datetime',\n",
    "                         \"Trip_Dropoff_DateTime\":\"Dropoff_Datetime\",\n",
    "                        },inplace = True)\n",
    "\n",
    "    # Select necessary columns\n",
    "    cleaned_df_2009 = cleaned_df_2009.loc[:, [\"Pickup_Datetime\",\"Trip_Distance\",\"Start_Lon\",\n",
    "                    \"Start_Lat\",\"End_Lon\",\"End_Lat\",\"Tip_Amt\"]]\n",
    "\n",
    "    # Drop NA\n",
    "    cleaned_df_2009.dropna(inplace=True)\n",
    "\n",
    "    return cleaned_df_2009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ee53b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_2009(year: int, sample_size: int):\n",
    "    \"\"\"\n",
    "    Process the data for the specified year and sample size.\n",
    "\n",
    "    :param year: The year of the data to process.\n",
    "    :param sample_size: The sample size to use for each month's data.\n",
    "    \"\"\"\n",
    "    # Define the file paths for each month for the specified year\n",
    "    file_paths = [f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/yellow_tripdata_{year}-{month:02}.parquet' for month in range(1, 13)]\n",
    "\n",
    "    all_dfs_2009 = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Load the entire dataset\n",
    "        entire_df_2009 = pd.read_parquet(file_path)\n",
    "\n",
    "        # Process the entire dataset\n",
    "        cleaned_entire_df_2009 = process_dataframe_2009(entire_df_2009)\n",
    "\n",
    "        # Sample the cleaned dataframe\n",
    "        sampled_df_2009 = cleaned_entire_df_2009.sample(n=sample_size, random_state=42)\n",
    "\n",
    "        all_dfs_2009.append(sampled_df_2009)\n",
    "\n",
    "    final_df_2009 = pd.concat(all_dfs_2009, ignore_index=True)\n",
    "    output_file_path = f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_{year}.parquet'\n",
    "    final_df_2009.to_parquet(output_file_path)\n",
    "\n",
    "# Set the sample size\n",
    "sample_size = 2500\n",
    "\n",
    "# Process the data for the year 2009\n",
    "process_data_2009(2009, sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1883cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data in cleaned_yellow_tripdata_2009.parquet: 30000\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_2009.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "print(\"Length of the data in cleaned_yellow_tripdata_2009.parquet:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be7502a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function \n",
    "import os\n",
    "\n",
    "def test_process_data_2009():\n",
    "    \"\"\"\n",
    "    Test the process_data function for years 2009\n",
    "    \"\"\"\n",
    "    sample_size = 10\n",
    "    year = 2009\n",
    "    process_data_2009(year, sample_size)\n",
    "    \n",
    "    output_file_path = f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/test_cleaned_yellow_tripdata_{year}.parquet'\n",
    "    \n",
    "    # Check if the output file is created\n",
    "    assert os.path.exists(output_file_path)\n",
    "    \n",
    "    # Load the output file and check the number of rows\n",
    "    test_df = pd.read_parquet(output_file_path)\n",
    "    assert len(test_df) == sample_size * 12  # 12 months of data\n",
    "\n",
    "# Run the test function\n",
    "test_process_data_2009()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf0dba6",
   "metadata": {},
   "source": [
    "#### clean the data for year 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c8e3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_2010(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the dataframe for year 2010.\n",
    "\n",
    "    :param df: The input dataframe to clean.\n",
    "    :return: The cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # Drop trips with zero passenger count\n",
    "    df = df[df['passenger_count'] != 0]\n",
    "\n",
    "    # Drop trips with no fare\n",
    "    df = df[df['fare_amount'] != 0]\n",
    "    df = df[df['tip_amount'] != 0]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[\"vendor_id\", \"passenger_count\", \"rate_code\", \"dropoff_datetime\", \"fare_amount\",\n",
    "                          \"store_and_fwd_flag\", \"payment_type\", \"surcharge\", \"mta_tax\", \"tolls_amount\", \"total_amount\"])\n",
    "\n",
    "    df.rename(columns={'pickup_datetime': 'Pickup_Datetime',\n",
    "                       \"dropoff_datetime\": \"Dropoff_Datetime\",\n",
    "                       \"trip_distance\": \"Trip_Distance\",\n",
    "                       \"fare_amount\": \"Fare_Amt\",\n",
    "                       \"tip_amount\": \"Tip_Amt\",\n",
    "                       \"pickup_longitude\": \"Start_Lon\",\n",
    "                       \"pickup_latitude\": \"Start_Lat\",\n",
    "                       \"dropoff_longitude\": \"End_Lon\",\n",
    "                       \"dropoff_latitude\": \"End_Lat\"\n",
    "                       }, inplace=True)\n",
    "\n",
    "    # Filter out trips that start and/or end outside of the defined latitude/longitude bounds\n",
    "    # Define the latitude/longitude bounds\n",
    "    lat_min, lat_max = 40.560445, 40.908524\n",
    "    lng_min, lng_max = -74.242330, -73.717047\n",
    "    df = df[(df['Start_Lat'] >= lat_min) & (df['Start_Lat'] <= lat_max) &\n",
    "            (df['End_Lat'] >= lat_min) & (df['End_Lat'] <= lat_max) &\n",
    "            (df['Start_Lon'] >= lng_min) & (df['Start_Lon'] <= lng_max) &\n",
    "            (df['End_Lon'] >= lng_min) & (df['End_Lon'] <= lng_max)]\n",
    "\n",
    "    # Filter out rows with the same pickup and dropoff location\n",
    "    df = df[(df['Start_Lon'] != df['End_Lon']) & (df['Start_Lat'] != df['End_Lat'])]\n",
    "\n",
    "    # Select necessary columns\n",
    "    df = df.loc[:, [\"Pickup_Datetime\", \"Trip_Distance\", \"Start_Lon\",\n",
    "                    \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Tip_Amt\"]]\n",
    "\n",
    "    # Remove NaN\n",
    "    df.dropna()\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f9c35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_2010(year: int, sample_size: int):\n",
    "    \"\"\"\n",
    "    Process the data for the specified year and sample size.\n",
    "\n",
    "    :param year: The year of the data to process.\n",
    "    :param sample_size: The sample size to use for each month's data.\n",
    "    \"\"\"\n",
    "    # Define the file paths for each month for the specified year\n",
    "    file_paths = [f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/yellow_tripdata_{year}-{month:02}.parquet' for month in range(1, 13)]\n",
    "\n",
    "    all_dfs_2010 = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Load the entire dataset\n",
    "        entire_df_2010 = pd.read_parquet(file_path)\n",
    "\n",
    "        # Process the entire dataset\n",
    "        cleaned_entire_df_2010 = process_dataframe_2010(entire_df_2010)\n",
    "\n",
    "        # Sample the cleaned dataframe\n",
    "        sampled_df_2010 = cleaned_entire_df_2010.sample(n=sample_size, random_state=42)\n",
    "\n",
    "        all_dfs_2010.append(sampled_df_2010)\n",
    "\n",
    "    final_df_2010 = pd.concat(all_dfs_2010, ignore_index=True)\n",
    "    output_file_path = f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_{year}.parquet'\n",
    "    final_df_2010.to_parquet(output_file_path)\n",
    "\n",
    "#Set the sample size\n",
    "sample_size = 2500\n",
    "\n",
    "#Process the data for the year 2010\n",
    "process_data_2010(2010, sample_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62daabd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data in cleaned_yellow_tripdata_2010.parquet: 30000\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_2010.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "print(\"Length of the data in cleaned_yellow_tripdata_2010.parquet:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94b9ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function \n",
    "import os\n",
    "\n",
    "def test_process_data_2010():\n",
    "    \"\"\"\n",
    "    Test the process_data function for years 2010\n",
    "    \"\"\"\n",
    "    sample_size = 10\n",
    "    year = 2010\n",
    "    process_data_2010(year, sample_size)\n",
    "    \n",
    "    output_file_path = f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/test_cleaned_yellow_tripdata_{year}.parquet'\n",
    "    \n",
    "    # Check if the output file is created\n",
    "    assert os.path.exists(output_file_path)\n",
    "    \n",
    "    # Load the output file and check the number of rows\n",
    "    test_df = pd.read_parquet(output_file_path)\n",
    "    assert len(test_df) == sample_size * 12  # 12 months of data\n",
    "\n",
    "# Run the test function\n",
    "test_process_data_2010()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e325a",
   "metadata": {},
   "source": [
    "#### clean the data for year 2011-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed11c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_2011_2014(raw_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and preprocess the given taxi data DataFrame.\n",
    "\n",
    "    :param raw_data: Input taxi data DataFrame.\n",
    "    :return: Cleaned and preprocessed taxi data DataFrame.\n",
    "    \"\"\"\n",
    "    # Drop trips with zero passenger count\n",
    "    valid_passengers = raw_data[raw_data['passenger_count'] != 0]\n",
    "\n",
    "    # Drop trips with no fare\n",
    "    valid_fare = valid_passengers[valid_passengers['fare_amount'] != 0]\n",
    "    valid_tip = valid_fare[valid_fare['tip_amount'] != 0]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    reduced_columns = valid_tip.drop(columns=[\"tolls_amount\", \"mta_tax\", \"extra\", \"VendorID\", \"passenger_count\", \"tpep_dropoff_datetime\",\n",
    "                          \"RatecodeID\", \"store_and_fwd_flag\", \"payment_type\", \"improvement_surcharge\",\n",
    "                          \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"fare_amount\"])\n",
    "\n",
    "    # Merge with location id and id\n",
    "    pickup_locations = reduced_columns.drop_duplicates().merge(zones, left_on='PULocationID', right_on=\"LocationID\",\n",
    "          how='left').rename(columns={'Lon':'Start_Lon', \"Lat\":\"Start_Lat\"}).drop(columns=[\"PULocationID\", \"LocationID\"])\n",
    "    cleaned_data = pickup_locations.merge(zones, left_on='DOLocationID', right_on=\"LocationID\", how='left'\n",
    "          ).rename(columns={'Lon':'End_Lon', \"Lat\":\"End_Lat\"}).drop(columns=[\"DOLocationID\", \"LocationID\"])\n",
    "\n",
    "    cleaned_data.rename(columns={'tpep_pickup_datetime':'Pickup_Datetime',\n",
    "                                  \"trip_distance\" : \"Trip_Distance\",\n",
    "                                  \"tip_amount\" : \"Tip_Amt\"\n",
    "                                 }, inplace=True)\n",
    "\n",
    "    # Define the latitude/longitude bounds\n",
    "    lat_min, lat_max = 40.560445, 40.908524\n",
    "    lng_min, lng_max = -74.242330, -73.717047\n",
    "\n",
    "    cleaned_data = cleaned_data[(cleaned_data['Start_Lat'] >= lat_min) & (cleaned_data['Start_Lat'] <= lat_max) &\n",
    "                        (cleaned_data['End_Lat'] >= lat_min) & (cleaned_data['End_Lat'] <= lat_max) &\n",
    "                        (cleaned_data['Start_Lon'] >= lng_min) & (cleaned_data['Start_Lon'] <= lng_max) &\n",
    "                        (cleaned_data['End_Lon'] >= lng_min) & (cleaned_data['End_Lon'] <= lng_max)]\n",
    "\n",
    "    # Filter out rows with the same pickup and dropoff location\n",
    "    cleaned_data = cleaned_data[(cleaned_data['Start_Lon'] != cleaned_data['End_Lon']) &\n",
    "                                 (cleaned_data['Start_Lat'] != cleaned_data['End_Lat'])]\n",
    "\n",
    "    cleaned_data = cleaned_data.reindex(columns=[\"Pickup_Datetime\", \"Trip_Distance\", \"Start_Lon\",\n",
    "                                                 \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Tip_Amt\"])\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    cleaned_data.dropna(inplace=True)\n",
    "\n",
    "    return cleaned_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8bb351d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_2011_2014(start_year: int, end_year: int, sample_size: int) -> None:\n",
    "    \"\"\"\n",
    "    Process and save the cleaned taxi data for the specified years.\n",
    "\n",
    "    :param start_year: Starting year for the data.\n",
    "    :param end_year: Ending year for the data.\n",
    "    :param sample_size: Number of samples per month.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "\n",
    "    for year in range(start_year, end_year+1):\n",
    "        # Define the file paths for each month for the specified year\n",
    "        file_paths = [f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/yellow_tripdata_{year}-{month:02}.parquet' for month in range(1, 13)]\n",
    "\n",
    "        year_dfs = []\n",
    "        for file_path in file_paths:\n",
    "            # Load the entire dataset\n",
    "            entire_df = pd.read_parquet(file_path)\n",
    "\n",
    "            # Process the entire dataset\n",
    "            cleaned_entire_df = process_dataframe_2011_2014(entire_df)\n",
    "\n",
    "            # Sample the cleaned dataframe\n",
    "            sampled_df = cleaned_entire_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "            year_dfs.append(sampled_df)\n",
    "\n",
    "        # Concatenate all sampled dataframes for the year\n",
    "        year_df = pd.concat(year_dfs, ignore_index=True)\n",
    "\n",
    "        all_dfs.append(year_df)\n",
    "\n",
    "    # Concatenate all sampled dataframes for all years\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    output_file_path = f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_{start_year}-{end_year}.parquet'\n",
    "    final_df.to_parquet(output_file_path)\n",
    "\n",
    "start_year = 2011\n",
    "end_year = 2014\n",
    "sample_size = 2500\n",
    "\n",
    "process_data_2011_2014(start_year, end_year, sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04359648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data in cleaned_yellow_tripdata_2011-2014.parquet: 120000\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_2011-2014.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "print(\"Length of the data in cleaned_yellow_tripdata_2011-2014.parquet:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "368deb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process_data_2011_2014():\n",
    "    \"\"\"\n",
    "    Test the process_data function for years 2011-2014\n",
    "    \"\"\"\n",
    "    start_year = 2011\n",
    "    end_year = 2014\n",
    "    sample_size = 10\n",
    "    process_data_2011_2014(start_year, end_year, sample_size)\n",
    "\n",
    "    output_file_path = f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/test_cleaned_yellow_tripdata_{start_year}-{end_year}.parquet'\n",
    "\n",
    "    # Check if the output file is created\n",
    "    assert os.path.exists(output_file_path)\n",
    "\n",
    "    # Load the output file and check the number of rows\n",
    "    test_df = pd.read_parquet(output_file_path)\n",
    "    total_months = (end_year - start_year + 1) * 12\n",
    "    assert len(test_df) == sample_size * total_months  # total months of data\n",
    "\n",
    "# Run the test function\n",
    "test_process_data_2011_2014()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8168515",
   "metadata": {},
   "source": [
    "#### clean the data for year 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a4edaa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_2015(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the dataframe for year 2015.\n",
    "\n",
    "    :param df: The input dataframe\n",
    "    :return: The cleaned dataframe\n",
    "    \"\"\"\n",
    "    # Drop trips with zero passenger count\n",
    "    df = df[df['passenger_count'] != 0]\n",
    "    \n",
    "    # Drop trips with no fare\n",
    "    df = df[df['fare_amount'] != 0]\n",
    "    df = df[df['tip_amount'] != 0]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df.drop(columns=[\"tolls_amount\", \"mta_tax\", \"extra\", \"VendorID\", \"passenger_count\", \"tpep_dropoff_datetime\",\n",
    "                     \"RatecodeID\", \"store_and_fwd_flag\", \"payment_type\", \"improvement_surcharge\",\n",
    "                     \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"fare_amount\"], inplace=True)\n",
    "\n",
    "    # Combine with location id and id\n",
    "    df2 = df.drop_duplicates().merge(zones, left_on='PULocationID', right_on=\"LocationID\",\n",
    "                                     how='left').rename(columns={'Lon': 'Start_Lon', \"Lat\": \"Start_Lat\"}).drop(\n",
    "        columns=[\"PULocationID\", \"LocationID\"])\n",
    "    clean_data = df2.merge(zones, left_on='DOLocationID', right_on=\"LocationID\", how='inner'\n",
    "                           ).rename(columns={'Lon': 'End_Lon', \"Lat\": \"End_Lat\"}).drop(columns=[\"DOLocationID\", \"LocationID\"])\n",
    "\n",
    "    clean_data.rename(columns={'tpep_pickup_datetime': 'Pickup_Datetime',\n",
    "                               \"tpep_dropoff_datetime\": \"Dropoff_Datetime\",\n",
    "                               \"trip_distance\": \"Trip_Distance\",\n",
    "                               \"fare_amount\": \"Fare_Amt\",\n",
    "                               \"tip_amount\": \"Tip_Amt\"\n",
    "                               }, inplace=True)\n",
    "\n",
    "    # Define the latitude/longitude bounds\n",
    "    lat_min, lat_max = 40.560445, 40.908524\n",
    "    lng_min, lng_max = -74.242330, -73.717047\n",
    "    # Filter out trips that start and/or end outside of the defined latitude/longitude bounds\n",
    "    clean_data = clean_data[(clean_data['Start_Lat'] >= lat_min) & (clean_data['Start_Lat'] <= lat_max) &\n",
    "                            (clean_data['End_Lat'] >= lat_min) & (clean_data['End_Lat'] <= lat_max) &\n",
    "                            (clean_data['Start_Lon'] >= lng_min) & (clean_data['Start_Lon'] <= lng_max) &\n",
    "                            (clean_data['End_Lon'] >= lng_min) & (clean_data['End_Lon'] <= lng_max)]\n",
    "\n",
    "    # Filter out rows with the same pickup and dropoff location\n",
    "    clean_data = clean_data[(clean_data['Start_Lon'] != clean_data['End_Lon']) &\n",
    "                            (clean_data['Start_Lat'] != clean_data['End_Lat'])]\n",
    "\n",
    "    clean_data = clean_data[[\"Pickup_Datetime\", \"Trip_Distance\", \"Start_Lon\",\n",
    "                             \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Tip_Amt\"]]\n",
    "\n",
    "    clean_data.dropna(inplace=True)\n",
    "    return clean_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e33a2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_2015(year: int, sample_size: int) -> None:\n",
    "    \"\"\"\n",
    "    Process data for the year 2015.\n",
    "\n",
    "    :param year: The year of data to process\n",
    "    :param sample_size: The number of samples to take for each month\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Define the file paths for each month for the specified year\n",
    "    file_paths = [f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/yellow_tripdata_{year}-{month:02}.parquet' for month in range(1, 7)]\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Load the entire dataset\n",
    "        entire_df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Process the entire dataset\n",
    "        cleaned_entire_df = process_dataframe_2015(entire_df)\n",
    "\n",
    "        # Sample the cleaned dataframe\n",
    "        sampled_df = cleaned_entire_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "        all_dfs.append(sampled_df)\n",
    "\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    output_file_path = f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_{year}.parquet'\n",
    "    final_df.to_parquet(output_file_path)\n",
    "\n",
    "# Set the sample size\n",
    "sample_size = 2500\n",
    "\n",
    "# Process the data for the year 2015\n",
    "process_data_2015(2015, sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1a3d20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data in cleaned_yellow_tripdata_2015.parquet: 15000\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_2015.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "print(\"Length of the data in cleaned_yellow_tripdata_2015.parquet:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67eb67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process_data_2015():\n",
    "    \"\"\"\n",
    "    Test the process_data function for the year 2015\n",
    "    \"\"\"\n",
    "    year = 2015\n",
    "    sample_size = 10\n",
    "    process_data_2015(year, sample_size)\n",
    "\n",
    "    output_file_path = f'/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/test_cleaned_yellow_tripdata_{year}.parquet'\n",
    "\n",
    "    # Check if the output file is created\n",
    "    assert os.path.exists(output_file_path)\n",
    "\n",
    "    # Load the output file and check the number of rows\n",
    "    test_df = pd.read_parquet(output_file_path)\n",
    "    total_months = 6  # Since the data for 2015 is available for 6 months only\n",
    "    assert len(test_df) == sample_size * total_months  # total months of data\n",
    "\n",
    "# Run the test function\n",
    "test_process_data_2015()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1455ac3",
   "metadata": {},
   "source": [
    "### Combine all the cleaned yellow taxi data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fce03a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concatenate_parquets(file_paths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Parquet files from the given file paths and concatenate them into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_paths (List[str]): List of file paths to the Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A concatenated DataFrame containing data from all the Parquet files.\n",
    "    \"\"\"\n",
    "    # Load all the Parquet files into a list of dataframes\n",
    "    dfs = [pd.read_parquet(f) for f in file_paths]\n",
    "\n",
    "    # Concatenate all the dataframes together\n",
    "    combined_data = pd.concat(dfs)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def sort_and_reset_index(df: pd.DataFrame, datetime_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sort the DataFrame by a datetime column and reset the index.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        datetime_column (str): The name of the datetime column to sort by.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The sorted DataFrame with reset index.\n",
    "    \"\"\"\n",
    "    # Convert the integer column to datetime format\n",
    "    df[datetime_column] = pd.to_datetime(df[datetime_column])\n",
    "\n",
    "    # Sort the DataFrame by the datetime column\n",
    "    df = df.sort_values(by=datetime_column)\n",
    "\n",
    "    # Reset the index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# List all the file paths\n",
    "file_list = [\n",
    "    \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_2009.parquet\",\n",
    "    \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_2010.parquet\",\n",
    "    \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_2011-2014.parquet\",\n",
    "    \"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/cleaned_yellow_tripdata_2015.parquet\"\n",
    "]\n",
    "\n",
    "# Load and concatenate the Parquet files\n",
    "yellow_taxi_data = load_and_concatenate_parquets(file_list)\n",
    "\n",
    "# Sort the DataFrame and reset the index\n",
    "yellow_taxi_data = sort_and_reset_index(yellow_taxi_data, 'Pickup_Datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4fc1636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195000\n",
      "      Pickup_Datetime  Trip_Distance  Start_Lon  Start_Lat    End_Lon  \\\n",
      "0 2009-01-01 01:17:09            3.7 -73.980560  40.730467 -73.989196   \n",
      "1 2009-01-01 01:24:09            2.6 -73.978789  40.777265 -73.971543   \n",
      "2 2009-01-01 01:38:29            0.5 -73.989369  40.735944 -73.987685   \n",
      "3 2009-01-01 02:29:16            3.1 -73.987969  40.718590 -73.938873   \n",
      "4 2009-01-01 02:45:46            5.7 -73.948401  40.809091 -73.987866   \n",
      "\n",
      "     End_Lat  Tip_Amt  \n",
      "0  40.763682     1.50  \n",
      "1  40.751308     3.00  \n",
      "2  40.773752     2.35  \n",
      "3  40.707808     1.83  \n",
      "4  40.748045     3.00  \n",
      "      Pickup_Datetime  pickup_hour  day_of_week  Start_Lon  Start_Lat  \\\n",
      "0 2009-01-01 01:15:22            1            3 -73.981918  40.779456   \n",
      "1 2009-01-01 01:59:17            1            3 -73.983759  40.721389   \n",
      "2 2009-01-01 02:05:03            2            3 -73.956635  40.771254   \n",
      "3 2009-01-01 02:09:13            2            3 -73.984605  40.728020   \n",
      "4 2009-01-01 02:13:41            2            3 -73.980127  40.737425   \n",
      "\n",
      "     End_Lon    End_Lat  Trip_Distance  \n",
      "0 -73.957685  40.771043       2.245470  \n",
      "1 -73.994833  40.687179       3.918071  \n",
      "2 -73.991528  40.749778       3.787925  \n",
      "3 -73.955746  40.776830       5.948824  \n",
      "4 -74.009544  40.726025       2.784896  \n",
      "192828\n"
     ]
    }
   ],
   "source": [
    "print(len(yellow_taxi_data))\n",
    "print(yellow_taxi_data.head(5))\n",
    "\n",
    "print(uber_data.head())\n",
    "print(len(uber_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3978a",
   "metadata": {},
   "source": [
    "## Processing weather data\n",
    "1. We need 'DATE','HourlyPrecipitation','HourlyWindSpeed'\n",
    "2. filter the time with minutes 51 as the daily data\n",
    "3. replace null with 0 and T with 0 for the 'HourlyPrecipitation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c31727c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:13: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:13: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:13: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:13: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:13: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:13: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:13: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].fillna(0).replace('T', 0).str.extract('(\\d+\\.?\\d*)').astype(float)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace('T', '0')\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather['HourlyWindSpeed'] = weather['HourlyWindSpeed'].replace('T', '0')\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].fillna(0).astype(float)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2932835114.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather['HourlyWindSpeed'] = weather['HourlyWindSpeed'].fillna(0).astype(float)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def read_weather_data(file_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read CSV files into a DataFrame, filter the required columns,\n",
    "    and concatenate all the filtered DataFrames together.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file)\n",
    "        df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "        df_filtered = df[['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "        dfs.append(df_filtered)\n",
    "    weather = pd.concat(dfs)\n",
    "    return weather\n",
    "\n",
    "\n",
    "def filter_weather_data(weather: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep only rows with minute equal to 51, reset index, and modify the 'HourlyPrecipitation' column.\n",
    "    \"\"\"\n",
    "    weather = weather[weather['DATE'].apply(lambda x: x.minute) == 51]\n",
    "    weather.reset_index(drop=True, inplace=True)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].fillna(0).replace('T', 0).str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace('T', '0')\n",
    "    weather['HourlyWindSpeed'] = weather['HourlyWindSpeed'].replace('T', '0')\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].fillna(0).astype(float)\n",
    "    weather['HourlyWindSpeed'] = weather['HourlyWindSpeed'].fillna(0).astype(float)\n",
    "    return weather\n",
    "\n",
    "\n",
    "def save_weather_data(weather: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the modified DataFrame as a new CSV file.\n",
    "    \"\"\"\n",
    "    weather.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "# List all the CSV file paths for the years 2009-2015\n",
    "file_list = []\n",
    "for year in range(2009, 2016):\n",
    "    file_list.extend(glob.glob(f\"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/weather_data/*{year}*_weather.csv\"))\n",
    "\n",
    "# Read all CSV files and concatenate the DataFrames\n",
    "weather = read_weather_data(file_list)\n",
    "\n",
    "# Apply necessary transformations\n",
    "weather = filter_weather_data(weather)\n",
    "\n",
    "# Save the modified DataFrame as a new CSV file\n",
    "save_weather_data(weather, '/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/weather_data/hourly_weather_2009_2015.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "671d9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file exists in the specified file path\n",
    "\n",
    "def check_file_exists(file_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a file exists in the given file path.\n",
    "\n",
    "    Args:\n",
    "        file_path: The file path to check.\n",
    "\n",
    "    Returns:\n",
    "        True if the file exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return os.path.isfile('/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/weather_data/hourly_weather_2009_2015.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03336c0c",
   "metadata": {},
   "source": [
    "## daily data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a59221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2703566846.py:22: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2703566846.py:22: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2703566846.py:22: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2703566846.py:22: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2703566846.py:22: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2703566846.py:22: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0q/hysr6_zj3pz7tbmdxkl32ns00000gp/T/ipykernel_34576/2703566846.py:22: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def read_weather_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read hourly weather data from CSV files for the years 2009-2015 and filter the required columns.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path where the weather data CSV files are located.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the filtered weather data.\n",
    "    \"\"\"\n",
    "    # List all the CSV file paths for the yers 2009-2015\n",
    "    file_list = []\n",
    "    for year in range(2009, 2016):\n",
    "        file_list.extend(glob.glob(f\"{file_path}/*{year}*_weather.csv\"))\n",
    "\n",
    "    # Read each CSV file into a DataFrame, filter the required columns, and store them in a list\n",
    "    dfs = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file)\n",
    "        df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "        df_filtered = df[['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "        dfs.append(df_filtered)\n",
    "\n",
    "    # Concatenate all the filtered DataFrames together\n",
    "    weather = pd.concat(dfs)\n",
    "\n",
    "    # Convert DATE column to datetime\n",
    "    weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "\n",
    "    # Keep only rows with minute equal to 51\n",
    "    weather = weather[weather['DATE'].apply(lambda x: x.minute) == 51]\n",
    "\n",
    "    # Reset index\n",
    "    weather.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Drop the 's' at the end of the HourlyWindSpeed column\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].fillna(0).replace('T', 0).str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "    # Replace 'T' with 0 in HourlyPrecipitation and HourlyWindSpeed columns\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].replace('T', '0')\n",
    "    weather['HourlyWindSpeed'] = weather['HourlyWindSpeed'].replace('T', '0')\n",
    "\n",
    "    # Convert HourlyPrecipitation and HourlyWindSpeed columns to float\n",
    "    weather['HourlyPrecipitation'] = weather['HourlyPrecipitation'].fillna(0).astype(float)\n",
    "    weather['HourlyWindSpeed'] = weather['HourlyWindSpeed'].fillna(0).astype(float)\n",
    "\n",
    "    return weather\n",
    "\n",
    "\n",
    "def get_daily_weather_data(weather_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group the hourly weather data by day and calculate the mean precipitation and wind speed.\n",
    "\n",
    "    Args:\n",
    "        weather_data (pd.DataFrame): A pandas DataFrame containing the hourly weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the daily weather data.\n",
    "    \"\"\"\n",
    "    # Convert DATE column to date\n",
    "    weather_data['DATE'] = weather_data['DATE'].dt.date\n",
    "\n",
    "    # Group by date and calculate mean precipitation and wind speed\n",
    "    precipitation = weather_data.groupby(['DATE']).mean()['HourlyPrecipitation']\n",
    "    wind_speed = weather_data.groupby(['DATE']).mean()['HourlyWindSpeed']\n",
    "\n",
    "    # Concatenate mean precipitation and wind speed into a new DataFrame\n",
    "    daily_weather = pd.concat((precipitation, wind_speed), axis=1)\n",
    "\n",
    "    # Rename columns\n",
    "    daily_weather.rename(columns={\"HourlyPrecipitation\": \"DailyPrecipitation\", \"HourlyWindSpeed\": \"DailyWindSpeed\"}, inplace=True)\n",
    "\n",
    "    # Reset index\n",
    "    daily_weather = daily_weather.reset_index()\n",
    "\n",
    "    return daily_weather\n",
    "\n",
    "# Read the weather data from CSV files\n",
    "weather_data = read_weather_data(\"/Users/Yolanda/CU2023spring/4501 tools of python/project/yellow_taxi_data/weather_data\")\n",
    "\n",
    "# Group the hourly weather data by day and calculate the mean precipitation and wind speed\n",
    "daily_weather_data = get_daily_weather_data(weather_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62fd116",
   "metadata": {},
   "source": [
    "## All datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76279f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Pickup_Datetime  pickup_hour  day_of_week  Start_Lon  Start_Lat  \\\n",
      "0      2009-01-01 01:15:22            1            3 -73.981918  40.779456   \n",
      "1      2009-01-01 01:59:17            1            3 -73.983759  40.721389   \n",
      "2      2009-01-01 02:05:03            2            3 -73.956635  40.771254   \n",
      "3      2009-01-01 02:09:13            2            3 -73.984605  40.728020   \n",
      "4      2009-01-01 02:13:41            2            3 -73.980127  40.737425   \n",
      "...                    ...          ...          ...        ...        ...   \n",
      "192823 2015-06-30 22:57:53           22            1 -73.971703  40.782207   \n",
      "192824 2015-06-30 23:16:42           23            1 -74.001099  40.730961   \n",
      "192825 2015-06-30 23:31:06           23            1 -73.999962  40.733135   \n",
      "192826 2015-06-30 23:33:33           23            1 -73.980988  40.762020   \n",
      "192827 2015-06-30 23:40:39           23            1 -73.984795  40.751411   \n",
      "\n",
      "          End_Lon    End_Lat  Trip_Distance  \n",
      "0      -73.957685  40.771043       2.245470  \n",
      "1      -73.994833  40.687179       3.918071  \n",
      "2      -73.991528  40.749778       3.787925  \n",
      "3      -73.955746  40.776830       5.948824  \n",
      "4      -74.009544  40.726025       2.784896  \n",
      "...           ...        ...            ...  \n",
      "192823 -73.943680  40.827991       5.612535  \n",
      "192824 -73.957123  40.806908       9.224129  \n",
      "192825 -73.962448  40.773041       5.449152  \n",
      "192826 -73.960083  40.770531       1.999366  \n",
      "192827 -73.927765  40.706287       6.949799  \n",
      "\n",
      "[192828 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(uber_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aaa7a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Pickup_Datetime  Trip_Distance  Start_Lon  Start_Lat    End_Lon  \\\n",
      "0      2009-01-01 01:17:09           3.70 -73.980560  40.730467 -73.989196   \n",
      "1      2009-01-01 01:24:09           2.60 -73.978789  40.777265 -73.971543   \n",
      "2      2009-01-01 01:38:29           0.50 -73.989369  40.735944 -73.987685   \n",
      "3      2009-01-01 02:29:16           3.10 -73.987969  40.718590 -73.938873   \n",
      "4      2009-01-01 02:45:46           5.70 -73.948401  40.809091 -73.987866   \n",
      "...                    ...            ...        ...        ...        ...   \n",
      "194995 2015-06-30 23:21:46           1.23 -73.988787  40.753513 -73.989845   \n",
      "194996 2015-06-30 23:26:10           1.46 -73.946510  40.775932 -73.959635   \n",
      "194997 2015-06-30 23:41:40           1.15 -73.999917  40.748428 -74.002875   \n",
      "194998 2015-06-30 23:43:46           3.30 -74.002875  40.734576 -73.986114   \n",
      "194999 2015-06-30 23:54:28           2.90 -74.007486  40.726290 -73.984196   \n",
      "\n",
      "          End_Lat  Tip_Amt  \n",
      "0       40.763682     1.50  \n",
      "1       40.751308     3.00  \n",
      "2       40.773752     2.35  \n",
      "3       40.707808     1.83  \n",
      "4       40.748045     3.00  \n",
      "...           ...      ...  \n",
      "194995  40.762253     1.36  \n",
      "194996  40.766948     2.08  \n",
      "194997  40.734576     2.34  \n",
      "194998  40.685634     2.66  \n",
      "194999  40.759818     2.55  \n",
      "\n",
      "[195000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(yellow_taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f2f61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     DATE  HourlyPrecipitation  HourlyWindSpeed\n",
      "0     2009-01-01 00:51:00                  0.0             18.0\n",
      "1     2009-01-01 01:51:00                  0.0             18.0\n",
      "2     2009-01-01 02:51:00                  0.0             18.0\n",
      "3     2009-01-01 03:51:00                  0.0              8.0\n",
      "4     2009-01-01 04:51:00                  0.0             11.0\n",
      "...                   ...                  ...              ...\n",
      "60306 2015-12-31 19:51:00                  0.0              6.0\n",
      "60307 2015-12-31 20:51:00                  0.0             10.0\n",
      "60308 2015-12-31 21:51:00                  0.0              0.0\n",
      "60309 2015-12-31 22:51:00                  0.0              7.0\n",
      "60310 2015-12-31 23:51:00                  0.0              5.0\n",
      "\n",
      "[60311 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a1323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DATE  DailyPrecipitation  DailyWindSpeed\n",
      "0     2009-01-01            0.000000       11.041667\n",
      "1     2009-01-02            0.000000        6.083333\n",
      "2     2009-01-03            0.000000        9.875000\n",
      "3     2009-01-04            0.000000        7.416667\n",
      "4     2009-01-05            0.000000        7.000000\n",
      "...          ...                 ...             ...\n",
      "2546  2015-12-27            0.005000        5.416667\n",
      "2547  2015-12-28            0.001250        7.791667\n",
      "2548  2015-12-29            0.016957        6.782609\n",
      "2549  2015-12-30            0.007917        4.250000\n",
      "2550  2015-12-31            0.001250        4.958333\n",
      "\n",
      "[2551 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(daily_weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d061d",
   "metadata": {},
   "source": [
    "# Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a2e93",
   "metadata": {},
   "source": [
    "##  load in preprocessed datasets.\n",
    "## create a schema.sql file that defines each table’s scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddfcf48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from typing import Dict\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "DATABASE_SCHEMA_FILE = \"database_schema.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a9677b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    HourlyPrecipitation FLOAT,\n",
    "    HourlyWindSpeed FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    DailyPrecipitation FLOAT,\n",
    "    DailyWindSpeed FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    Pickup_Datetime DATETIME,\n",
    "    Tip_Amt FLOAT,\n",
    "    Start_Lon FLOAT,\n",
    "    Start_Lat FLOAT,\n",
    "    End_Lon FLOAT,\n",
    "    End_Lat FLOAT,\n",
    "    Trip_Distance FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA= \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    Pickup_Datetime DATETIME,\n",
    "    Start_Lon FLOAT,\n",
    "    Start_Lat FLOAT,\n",
    "    End_Lon FLOAT,\n",
    "    End_Lat FLOAT,\n",
    "    Trip_Distance FLOAT,\n",
    "    day_of_week INTEGER,\n",
    "    pickup_hour INTEGER\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2106760",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Create database schema based on the schema file.\n",
    "    \"\"\"\n",
    "    with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "        f.write(HOURLY_WEATHER_SCHEMA + \";\\n\")\n",
    "        f.write(DAILY_WEATHER_SCHEMA + \";\\n\")\n",
    "        f.write(TAXI_TRIPS_SCHEMA + \";\\n\")\n",
    "        f.write(UBER_TRIPS_SCHEMA + \";\\n\")\n",
    "\n",
    "    # create the tables with the schema files\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "        connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "        connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "        connection.execute(UBER_TRIPS_SCHEMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "27576138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict: Dict[str, pd.DataFrame]) -> None:\n",
    "    \"\"\"\n",
    "    Writes data from a dictionary of table names to dataframes\n",
    "    to their respective tables in the SQLite database.\n",
    "\n",
    "    Args:\n",
    "        table_to_df_dict (Dict[str, pd.DataFrame]): A dictionary containing the table names and dataframes.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Write data from each dataframe to their respective tables in the database\n",
    "    for table_name, df in table_to_df_dict.items():\n",
    "        try:\n",
    "            df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "            print(f\"Data successfully written to table {table_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing data to table {table_name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75fcaab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to table hourly_weather.\n",
      "Data successfully written to table daily_weather.\n",
      "Data successfully written to table taxi_trips.\n",
      "Data successfully written to table uber_trips.\n"
     ]
    }
   ],
   "source": [
    "# Example dictionary with table names and dataframes\n",
    "tables_to_df_dict = {\n",
    "    \"hourly_weather\": weather,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "    \"taxi_trips\": yellow_taxi_data,\n",
    "    \"uber_trips\": uber_data\n",
    "}\n",
    "\n",
    "# Write the dataframes to the tables in the database\n",
    "write_dataframes_to_table(tables_to_df_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d28db2",
   "metadata": {},
   "source": [
    "## Test the database and written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c00f75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sqlite3 project.db < database_schema.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1ea160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             DATE  HourlyPrecipitation  HourlyWindSpeed\n",
      "0      2009-01-01 00:51:00.000000                  0.0             18.0\n",
      "1      2009-01-01 01:51:00.000000                  0.0             18.0\n",
      "2      2009-01-01 02:51:00.000000                  0.0             18.0\n",
      "3      2009-01-01 03:51:00.000000                  0.0              8.0\n",
      "4      2009-01-01 04:51:00.000000                  0.0             11.0\n",
      "...                           ...                  ...              ...\n",
      "60306  2015-12-31 19:51:00.000000                  0.0              6.0\n",
      "60307  2015-12-31 20:51:00.000000                  0.0             10.0\n",
      "60308  2015-12-31 21:51:00.000000                  0.0              0.0\n",
      "60309  2015-12-31 22:51:00.000000                  0.0              7.0\n",
      "60310  2015-12-31 23:51:00.000000                  0.0              5.0\n",
      "\n",
      "[60311 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Check that the data has been successfully written to the tables by running a sample query\n",
    "# Execute a query and read the results into a DataFrame object\n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM hourly_weather\", connection)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f4a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
